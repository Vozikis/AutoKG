# AutoKGÂ â€“ Generative AI Architectures for Structured Graphs  
<img src="figures/autoreg.png" alt="AutoKG Highâ€‘level Architecture" width="1080"/>

AutoKG is a **variationalâ€‘autoregressive model** for **multiâ€‘relational knowledgeâ€‘graph (KG) generation**.  
On the **IntelliGraphs** benchmark it surpasses classical and modern baselines (TransE, ComplEx, VAE, LDM) on:

* **Validity**
* **Novelty**
* **Compression efficiency**


We also experimented with 
* **graphâ€‘level diversity** and 
* **constrained generation**.


---

## ğŸ” TableÂ ofÂ Contents
1. [Overview](#overview)
2. [ProjectÂ Structure](#project-structure)
3. [QuickÂ Start](#quick-start)
4. [Methodology](#methodology)
5. [EvaluationÂ &Â Results](#evaluation--results)
6. [Datasets](#datasets)

---

## ğŸ“Œ Overview <a name="overview"/></a>

Generative models have achieved remarkable results in domains like image, text, and video synthesis. However, they continue to struggle with generating coherent symbolic structures such as knowledge graphs (KGs). AutoKG addresses this gap through a novel variational-autoregressive architecture designed to model the full joint distribution over triples. It transforms knowledge graphs into token sequences, enabling latent compression and autoregressive decoding to generate structurally valid, novel, and diverse multirelational graphs. 

In our evaluation, we compared AutoKG against a range of classical and modern generative baselines. 
### ğŸ”¬ Baseline Models

As baseline, apart from the models already available from the IntelliGraphs benchmark (**TransE**, **ComplEx**, and **Random**), we also implement a **Variational Autoencoder (VAE)** and a **Latent Diffusion Model (LDM)**.

- **TransE** models relations as a translation in the embedding space and enforces the principle that for each of the valid triples \((h, r, t)\), we have \(h + r \approx t\).
- **ComplEx** embeds relations and entities in the complex domain, helping the model to capture both symmetric and asymmetric patterns in the data.

These methods focus primarily on learning and optimizing a scoring function that is capable of distinguishing plausible from implausible triples. However, a key limitation is that they treat all triples independently of one another.

The **VAE** consists of three componentsâ€”an **encoder**, a **decoder**, and a **structural decoder**â€”and learns a latent representation of the entire graph. The decoder predicts entities from the latent vector, and a **RESCAL-based structural decoder** reconstructs the graph by scoring entity pairs for each relation.

In contrast, the **LDM** follows the same encoderâ€“decoderâ€“structural decoder architecture but introduces a **diffusion module** that gradually adds and removes noise in the latent space. The denoising process enables the model to learn a distribution over structured latent representations.

Both VAE and LDM demonstrate the benefits of latent modeling but generate entire graphs in a single step, limiting their ability to capture sequential dependencies.

**AutoKG** addresses this by combining a **variational latent space** with an **autoregressive decoder**, generating triples token-by-token, each conditioned on previous tokens and a latent vector \(z\). This allows AutoKG to achieve **superior performance across all evaluation metrics**.



All models are evaluated on five datasets from **IntelliGraphs** (`synâ€‘paths`, `synâ€‘types`, `synâ€‘tipr`, `wdâ€‘movies`, `wdâ€‘articles`).

---

## ğŸ“ Project Structure <a name="project-structure"/></a>

The repository contains scripts for training and evaluating models on five IntelliGraphs datasets, which include both synthetic (rule-based) and real-world graphs with varying complexity and structure.

```text
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ autoreg_paths.py        # AutoKG on SYNâ€‘PATHS
â”‚   â”œâ”€â”€ autoreg_types.py        # AutoKG on SYNâ€‘TYPES
â”‚   â”œâ”€â”€ autoreg_tipr.py         # AutoKG on SYNâ€‘TIPR
â”‚   â”œâ”€â”€ autoreg_movies.py       # AutoKG on WDâ€‘MOVIES
â”‚   â””â”€â”€ autoreg_articles.py     # AutoKG on WDâ€‘ARTICLES  
â”œâ”€â”€ checkpoints
â”œâ”€â”€ figures
â””â”€â”€ â€¦
```

---

## ğŸš€ Quick Start <a name="quick-start"/></a>

```bash
# 1. install core requirements
pip install -r requirements.txt

# 2. install IntelliGraphs (datasets & helpers)
pip install intelligraphs

# 3. train AutoKG on the Intelligraphs dataset of interest 
python src/autoreg_paths.py        # SYNâ€‘PATHS
python src/autoreg_types.py        # SYNâ€‘TYPES
python src/autoreg_tipr.py         # SYNâ€‘TIPR
python src/autoreg_movies.py       # WDâ€‘MOVIES
python src/autoreg_articles.py     # WDâ€‘ARTICLES
```

Each script  
*loads* the dataset, performs training, and stores checkpoints in `checkpoints/`.

---

## ğŸ§  Methodology <a name="methodology"/></a>

AutoKG is the first architecture to combine variational inference with autoregressive decoding for multirelational graph generation. Each triple element (subject, predicate, object) is tokenized into a shared vocabulary. The encoder compresses the tokenized graph into a latent vector z, and the decoder generates triple elements sequentially, conditioned on previous tokens and z. This setup allows the model to learn and control long-range dependencies across triples, producing semantically consistent and valid graphs.

### ğŸ—  AutoKG in one picture  

<img src="figures/autoreg.png" alt="AutoKG in one picture" width="720"/>


**AutoKG** is a **variational-autoregressive model** for structured **knowledge graph (KG) generation**.  
It learns to model the **joint probability** over full graphsâ€”not isolated triplesâ€”by transforming each graph into a token sequence and decoding it **autoregressively**, conditioned on a **latent vector** `z`.

<img src="figures/autoreg_gen.png" alt="AutoKG generation in an autoregressive manner" width="720"/>

---

### 1. ğŸ—ï¸ Architecture Overview

- **Encoder**: Transforms input KG (triples) into a latent vector `z âˆˆ â„áµˆ` using a Transformer encoder with type-specific token embeddings.
- **Latent Bottleneck**: A variational bottleneck regularized by KL divergence, encouraging compressed global representations.
- **Decoder**: Autoregressively generates tokens `xâ‚œ âˆˆ {s, p, o}`, conditioned on past tokens and `z` using masked self-attention and cross-attention.
- **Tokenization**: Graphs are serialized into sequences like: [Amsterdam, train_to, Utrecht]

- **Special tokens**: `PAD = 0`, `BOS = 1`, `EOS = 2`  
- **Vocabulary size**: `|V| = 3 + |E| + |R|`  
- **Padding** is used for fixed-length batching.

---

### 3. ğŸ§  Encoder

- **Transformer encoder layers** capture inter-triple dependencies.
- Latent vector `z` is sampled via reparameterization.

### 4. âœï¸ Decoder

- **Masked self-attention** over past tokens
- **Cross-attention** on the latent vector `z`
- Predicts the next token.

### 5. ğŸ¯ Training Objective

AutoKG maximizes the **Evidence Lower Bound (ELBO)**.



### âš™ï¸  Conditioning Mechanism  

> Give AutoKG a *partial* triple sequence â€” it will **complete** the graph while respecting constraints.

<img src="figures/cond_generation.png" alt="Stepâ€‘byâ€‘Step Conditioning of Graph Generation" width="780"/>

---

## ğŸ“Š EvaluationÂ &Â Results <a name="evaluation--results"/></a>

Our evaluation spans five datasets from IntelliGraphs. AutoKG achieves state-of-the-art validity and novelty, with near-perfect scores on all datasets. Unlike baselines, AutoKG retains diversity across samples and scales well to complex real-world graphs like wd-articles. Compression analysis confirms that AutoKG learns highly efficient latent representations, outperforming both VAEs and latent diffusion models.


### 1. Compession evaluation and validity and novelty for sampled from the latent space and conditioned on the test set graphs 


<img src="figures/compression.png" alt="Compression cost for different models and datasets. Bolded
entries highlight the performance of the best model" width="780"/>  
<img src="figures/results_latent.png" alt="Sampling $P(E)$ and $P(S \mid E)$ across datasets and models. Bolded entries highlight the performance of the best model." width="780"/>  
<img src="figures/results_test.png" alt="Sampling $P(S \mid E)$ only across datasets and models. Bolded entries highlight the performance of the best model." width="780"/>  


### 2. Fineâ€‘grained constraint satisfaction

| Metric | Stepâ€‘byâ€‘step behaviour |
|--------|-----------------------|
| **DiversityÂ Ratio** | <img src="figures/diversity_constraint.png" alt="Diversity Ratio per Step" width="370"/> |
| **NovelÂ Semantics** | <img src="figures/novel_constraing.png" alt="Novel Semantics per Step" width="370"/> |

â†’ Diversity decreases as conditioning increases (expected).

### 3. Compressionâ€“Validity tradeâ€‘off (synthetic datasets)

| **SYNâ€‘PATHS** | **SYNâ€‘TIPR** | **SYNâ€‘TYPES** |
|---------------|--------------|---------------|
| <img src="figures/syn-path-compression.png" alt="Compression SYNâ€‘PATHS" width="330"/> | <img src="figures/syn-tipr-compression.png" alt="Compression SYNâ€‘TIPR" width="330"/> | <img src="figures/syn-types-compression.png" alt="Compression SYNâ€‘TYPES" width="330"/> |
| <img src="figures/syn-path-valid.png"   alt="Validity SYNâ€‘PATHS"  width="330"/> | <img src="figures/syn-typr-valid.png"   alt="Validity SYNâ€‘TIPR"  width="330"/> | <img src="figures/syn-types-valid.png"  alt="Validity SYNâ€‘TYPES" width="330"/> |

* KLÂ â†’Â 0 at small latent sizes â‡’ information flows through **discrete autoregression** alone.  
* Novelty and validity degrade with wider latents.  

### 4. Diversity ratio
<img src="figures/diversity.png" alt="Graph Diversity of AutoKG Model (10,000 Random Latent Samples)" width="580"/>  


---

## ğŸ—‚  Datasets <a name="datasets"/></a>

All from **[IntelliGraphs](https://github.com/thiviyanT/IntelliGraphs)**.

| Name          | Type | Entities | Relations | Triples / graph |
|---------------|------|----------|-----------|-----------------|
| synâ€‘paths     | synthetic | 49  | 3 | â‰¤â€¯3 |
| synâ€‘types     | synthetic | 30  | 3 | â‰¤â€¯10 |
| synâ€‘tipr      | synthetic | 131 | 5 | â‰¤â€¯10 |
| wdâ€‘movies     | realâ€‘world | 24â€¯093 | 3 | â‰¤â€¯21 |
| wdâ€‘articles   | realâ€‘world | 60â€¯932 | 6 | â‰¤â€¯212 |

---